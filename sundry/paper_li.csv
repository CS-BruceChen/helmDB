_id,title,venue,year,keywords,n_citation,lang,authors,fos,page_start,page_end,volume,issue,issn,isbn,doi,pdf,url,abstract,references
5dc149a33a55acb75f3915ac,"An End-to-End Deep RL Framework for Task Arrangement in Crowdsourcing
  Platforms","{'sid': 'conf/icde', 't': 'C', 'raw': 'ICDE'}",2019,,0.0,,"[{'_id': '5626f00345cedb33986ab26b', 'name': 'Shan Caihua', 'org': 'THE UNIVERSITY OF HONG KONG ', 'orgs': ['THE UNIVERSITY OF HONG KONG '], 'orgid': '5f71b2961c455f439fe3ce46'}, {'_id': '53f43306dabfaedd74d7da8a', 'name': 'Mamoulis Nikos', 'org': 'University of Ioannina', 'orgs': ['University of Ioannina'], 'orgid': '5f71b4811c455f439fe4a742'}, {'name': 'Cheng Reynold', 'org': 'THE UNIVERSITY OF HONG KONG ', 'orgs': ['THE UNIVERSITY OF HONG KONG '], 'orgid': '5f71b2961c455f439fe3ce46'}, {'_id': '544861addabfae87b7e17147', 'name': 'Li Guoliang', 'org': 'Tsinghua University', 'orgs': ['Tsinghua University'], 'orgid': '5f71b2881c455f439fe3c860'}, {'_id': '562937c045cedb3398843d59', 'name': 'Li Xiang', 'org': 'THE UNIVERSITY OF HONG KONG ', 'orgs': ['THE UNIVERSITY OF HONG KONG '], 'orgid': '5f71b2961c455f439fe3ce46'}, {'name': 'Qian Yuqiu', 'org': '(  Tencent  Inc.)', 'orgs': ['(  Tencent  Inc.)'], 'orgid': '5f71b4341c455f439fe48558'}]","['Crowdsourcing', 'End-to-end principle', 'Computer science', 'Supervised learning', 'Artificial intelligence', 'Artificial neural network', 'Machine learning', 'Computation', 'Reinforcement learning']",49,60,,,,,10.1109/ICDE48307.2020.00012,https://static.aminer.cn/storage/pdf/arxiv/19/1911/1911.01030.pdf,"['https://arxiv.org/abs/1911.01030', 'https://dblp.org/rec/conf/icde/ShanMC0LQ20', 'https://doi.org/10.1109/ICDE48307.2020.00012', 'https://dblp.uni-trier.de/db/journals/corr/corr1911.html#abs-1911-01030']","  In this paper, we propose a Deep Reinforcement Learning (RL) framework for task arrangement, which is a critical problem for the success of crowdsourcing platforms. Previous works conduct the personalized recommendation of tasks to workers via supervised learning methods. However, the majority of them only consider the benefit of either workers or requesters independently. In addition, they cannot handle the dynamic environment and may produce sub-optimal results. To address these issues, we utilize Deep Q-Network (DQN), an RL-based method combined with a neural network to estimate the expected long-term return of recommending a task. DQN inherently considers the immediate and future reward simultaneously and can be updated in real-time to deal with evolving data and dynamic changes. Furthermore, we design two DQNs that capture the benefit of both workers and requesters and maximize the profit of the platform. To learn value functions in DQN effectively, we also propose novel state representations, carefully design the computation of Q values, and predict transition probabilities and future states. Experiments on synthetic and real datasets demonstrate the superior performance of our framework. ","['5736960b6e3b12023e51e3ea', '5f7ef3f59fced0a24bebb645', '56f6010e0cf2e9dcb5819c75', '5d68f4b8ed7e9c79265606f6', '5a9cb66717c44a376ffb8679', '599c7bf6601a182cd2777f05', '58437777ac44360f1083fb87', '573697c16e3b12023e6a24de', '5a260c8417c44a4ba8a31358', '53e9b31bb7602d9703de33f9', '5c4ecd697301396d1f044236', '5c8b4d794895d9cbc67f0e33', '53e99a85b7602d97022f8644', '5736960a6e3b12023e51d96d', '5ce2cf99ced107d4c631b9ee', '5cfa5b985ced2477cb3c50f0', '5736974d6e3b12023e63890e', '55323cac45cec66b6f9dc9ef', '5aed148b17c44a4438154fae', '53e99afeb7602d970238e1fc', '53e99fa9b7602d9702880bd1', '59ae3bf12bbe271c4c71bdcc', '53e99e8cb7602d970275320d', '57d063a2ac4436735428975d', '53e9a727b7602d97030602ae']"
